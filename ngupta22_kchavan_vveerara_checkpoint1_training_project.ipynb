{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT for Personalized Recipe Recommendation and Generating Visualizations with GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Batch: 1, Loss: 6.145355701446533\n",
      "Epoch: 20, Batch: 1, Loss: 4.667580604553223\n",
      "Epoch: 30, Batch: 1, Loss: 2.798748254776001\n",
      "Epoch: 40, Batch: 1, Loss: 1.0829516649246216\n",
      "Epoch: 50, Batch: 1, Loss: 0.3258196711540222\n",
      "Epoch: 60, Batch: 1, Loss: 0.18544597923755646\n",
      "Epoch: 70, Batch: 1, Loss: 0.08730714023113251\n",
      "Epoch: 80, Batch: 1, Loss: 0.11106064915657043\n",
      "Epoch: 90, Batch: 1, Loss: 0.056963592767715454\n",
      "Epoch: 100, Batch: 1, Loss: 0.05025213584303856\n",
      "Training complete! Model saved as 'trained_recipe_model_pytorch'\n"
     ]
    }
   ],
   "source": [
    "file_path = \"processed_recipes.txt\"\n",
    "\n",
    "text = open(file_path, \"r\").read()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Initialize GPT-2 tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to end-of-sequence token\n",
    "encoded_text = tokenizer.encode(text, max_length=1024, truncation=True, padding=\"max_length\")  # Tokenize and encode text\n",
    "encoded_tensor = torch.tensor(encoded_text).unsqueeze(0)  # Convert encoded text to tensor format and add batch dimension\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(encoded_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # Load pre-trained GPT-2 model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Training the model for 100 epochs\n",
    "for epoch in range(100):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch_input = batch[0].to(device)  # Move batch input to device\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(batch_input)  # Forward pass\n",
    "        predictions = outputs.logits[:, :-1]  # Remove last token from predictions\n",
    "        batch_input = batch_input[:, :-1]  # Remove last token from batch input to align dimensions\n",
    "        \n",
    "        loss = loss_fn(predictions.view(-1, predictions.size(-1)), batch_input.view(-1))  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:   \n",
    "        print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "model.save_pretrained(\"trained_recipe_model_pytorch\")  # Save trained model\n",
    "print(\"Training complete! Model saved as 'trained_recipe_model_pytorch'\")  # Print completion message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe suggestion 1: I want to make cranberry burritos. I want want make make MAKE MAKE Make Make Made Make Making Making Make Get Get Got Got Get\n",
      "\n",
      "1919202070707171 71 71 70 70704704704694694714714694664664464464646 46 464648483232 32 32 33 33 32 31 31 30 30\n",
      "...-.-.\n",
      " ( (((<<\n",
      ")) ) ) ( (( (((((( (( ((( ()(((((()( ((( (()(( (///././ and/ (.//(/(/ / //*//?/?/.,,,,.., 8 8 9 9 8 10 10 12 12 13 13 14 14 15 151515 15 2015 201520152015 2015 2016 2016 16 16 8 7 7 8 4 4 88899 9 10 20 20 25 25 26 26 24 24 25 27 27 28 28 29 29 30 31 32\n",
      " or\n",
      "Recipe suggestion 2: I want to make cranberry burritos.\n",
      "\n",
      "I'm not going to take take. I III I ( I ) ( ( ) )\n",
      " () ((()()(((((( (( (((( ( (( ( [ [ ] ] [ 1 1 2 2 3 3 4 4 5 5 4 6 6 66 66 68 68 69 69 70 70 71 71 7070702020 20 20\n",
      ",,..!!!!''\n",
      "//... [,,....... - - ( -      -  * * \n",
      "\n",
      "Recipe suggestion 3: I want to make cranberry burritos.   I I      \n",
      "   - - - --- \n",
      "\n",
      "I IIIIIIIIIIIII III III  IIIIIIIIIVIVIIIIVIIIIIVIIIXIXIVIXIVIIII II II I \n",
      " \n",
      "        '' \" \" \"\" \"\"\"\"\"\"\"\"\"\"\"\" \"\"\" \"\"\"\"\"\"\"\" \"\"=\"\"=\"\"=\"=\"/\"/\"=\"…\"…\"\"…\"……\"……\"…)…)……..…….….… … ………)...)…)…]…)!]!]']']''',',,, and and in in In In IN IN in the in a a an an a as as a of of a for for in at at in between between among among amongst amongst among amidst amidst amid amidst amongst amidst among amid amid amongst amid among throughout throughout amidst throughout amid throughout Throughout Throughout Through Through Throughout During During Throughout Despite Despite Throughout Additionally Additionally Furthermore Additionally Moreover Additionally Accordingly\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_path = \"trained_recipe_model_pytorch\"  # Path to the trained model directory\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Load the tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)  # Load the pre-trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available\n",
    "model.to(device) \n",
    "\n",
    "# Define input prompt\n",
    "input_prompt = \"I want to make cranberry burritos.\"\n",
    "\n",
    "# Tokenize input prompt\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate recipe suggestions\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,  # Maximum length of the generated text\n",
    "    num_return_sequences=3,  # Number of recipe suggestions to generate\n",
    "    no_repeat_ngram_size=2,  # Ensure generated sequences do not repeat n-grams of length 2\n",
    "    top_p=0.92,  # Probability threshold for nucleus sampling\n",
    "    temperature=0.85,  # Temperature for sampling\n",
    "    do_sample=True,  # Enable sampling from the output distribution\n",
    "    top_k=50,  # Top-k sampling parameter\n",
    "    early_stopping=False,  # Disable early stopping to enforce maximum length\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Pad token ID for end of sequence\n",
    ")\n",
    "\n",
    "# Decode and print recipe suggestions\n",
    "for i, recipe in enumerate(output):\n",
    "    print(f\"Recipe suggestion {i+1}: {tokenizer.decode(recipe, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://pytorch.org/tutorials/\n",
    "1. https://huggingface.co/transformers/\n",
    "1. https://huggingface.co/transformers/model_doc/gpt2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = \"processed_recipes_main.txt\"\n",
    "\n",
    "# Load the text file\n",
    "text = open(file_path, \"r\").read()\n",
    "\n",
    "# Tokenizer for GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(text, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "encoded_tensor = torch.tensor(encoded_text).unsqueeze(0)\n",
    "\n",
    "# Prepare training dataset\n",
    "dataset = torch.utils.data.TensorDataset(encoded_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# Create GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "training_loss_plt_arr = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0  # Track total loss for the epoch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch_input = batch[0].to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=batch_input, labels=batch_input)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Average epoch loss\n",
    "    epoch_loss /= len(dataloader)\n",
    "    \n",
    "    training_loss_plt_arr.append(epoch_loss)\n",
    "    \n",
    "    # Print training loss\n",
    "    if (epoch + 1) % 25 == 0:   \n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Loss: {epoch_loss:.5f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_recipe_GPT2_model\")\n",
    "\n",
    "print(\"Training complete! Model saved as 'trained_recipe_GPT2_model'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
