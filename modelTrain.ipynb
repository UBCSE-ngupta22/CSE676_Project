{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 9.368671417236328\n",
      "Epoch: 2, Batch: 1, Loss: 8.87807846069336\n",
      "Epoch: 3, Batch: 1, Loss: 8.512247085571289\n",
      "Epoch: 4, Batch: 1, Loss: 8.192841529846191\n",
      "Epoch: 5, Batch: 1, Loss: 7.700817108154297\n",
      "Epoch: 6, Batch: 1, Loss: 7.334148406982422\n",
      "Epoch: 7, Batch: 1, Loss: 6.919040203094482\n",
      "Epoch: 8, Batch: 1, Loss: 6.6689324378967285\n",
      "Epoch: 9, Batch: 1, Loss: 6.401946544647217\n",
      "Epoch: 10, Batch: 1, Loss: 6.205134868621826\n",
      "Training complete! Model saved as 'trained_recipe_model_pytorch'\n"
     ]
    }
   ],
   "source": [
    "file_path = \"processed_recipes.txt\"      # file path to take file as an input\n",
    "\n",
    "text = open(file_path, \"r\").read()       # loading the txt file and storing it as text\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")      # Using a pretrained tokenizer from GPT-2 Model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_text = tokenizer.encode(text, max_length=1024, truncation=True, padding=\"max_length\")     # Encoding text using tokenizer and setting max_length as 1024\n",
    "encoded_tensor = torch.tensor(encoded_text).unsqueeze(0)           # Converting encoded text to a tensor using torch.tensor and unsqueezing for increasing dimensions\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(encoded_tensor)           # Creating dataset using encoded tensor using the torch,utils library\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)    # Keeping the batch size as 8 for faster training\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")             # Creating a Pretrained GPT-2 Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     # Using CCR so to.device() is necessary\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)                # Using Adam optimizer and settign Learning Rate as 1e-5\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Using CrossEntropy Loss as Loss function\n",
    "\n",
    "# Trainign the model for 10 epochs\n",
    "for epoch in range(10):\n",
    "    for i, batch in enumerate(dataloader):                    # Putting the dataloader in batches of 8\n",
    "        batch_input = batch[0].to(device)                     # Moving data to device (CPU or GPU)\n",
    "        model.train()                                         # Applying training function to the model\n",
    "         \n",
    "        optimizer.zero_grad()                                 # Setting zero gradient function to the optimizer\n",
    "        \n",
    "        outputs = model(batch_input)                          # Getting the output for training parameters\n",
    "        predictions = outputs.logits[:, :-1]                  # Setting dimensions by removing last element\n",
    "        batch_input = batch_input[:, :-1]                     # Setting batch size decreasing by 1, as to adjust for loss function\n",
    "        \n",
    "        loss = loss_fn(predictions.view(-1, predictions.size(-1)), batch_input.view(-1))  # Flattening and calculating loss\n",
    "        loss.backward()                                       # Backpropagating loss\n",
    "        optimizer.step()   \n",
    "    print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")       # Printing epoch no, batch and loss \n",
    "\n",
    "model.save_pretrained(\"trained_recipe_model_pytorch\")         # Saving pretrained model\n",
    "\n",
    "print(\"Training complete! Model saved as 'trained_recipe_model_pytorch'\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
